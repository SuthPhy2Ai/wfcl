{
 "cells": [
  {
   "cell_type": "code",
   "id": "w3do4iiu4ar",
   "source": "# Add parent directory to path to import from src package\nimport sys\nfrom pathlib import Path\nsys.path.append(str(Path.cwd().parent))",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e93b861",
   "metadata": {},
   "outputs": [],
   "source": "import logging\nlogging.basicConfig(\n        format=\"%(asctime)s - %(levelname)s - %(name)s -   %(message)s\",\n        datefmt=\"%m/%d/%Y %H:%M:%S\",\n        level=logging.INFO,\n)\n\nimport e3nn.o3\nimport e3nn.nn\n\n\nimport glob\nimport os\nimport numpy as np\nimport copy as cpy\nimport torch\nimport random\nimport pandas as pd\nimport multiprocessing as mp\nfrom tqdm import  tqdm\nfrom torch.utils.data import DataLoader, random_split, Subset\n# from torch.utils.data import DataLoader\nfrom ase.db import connect\nfrom src.data.dataset import CharDataset, MyCollator\nfrom src.models.clip_model import CLIP, CLIPConfig, PointNetConfig\nfrom src.models.crystal_encoder import cry_config, CRY_ENCODER\n\nfrom pathlib import Path\nfrom typing import Union\n\nimport math\nimport time\nfrom torch.autograd import Variable\nfrom torchvision import transforms\nfrom torch.optim import AdamW\nfrom transformers import (\n    # AdamW,\n    SchedulerType,\n    get_scheduler,\n    set_seed,\n)\n\n\n\n\n# set the random seed\nseed = 42\nrandom.seed(seed)\nnp.random.seed(seed)\ntorch.manual_seed(seed)\ntorch.cuda.manual_seed_all(seed)\n\n# config\ndevice='cuda:1'\nnumEpochs = 1000 # number of epochs to train the GPT+PT model\nembeddingSize = 384 # the hidden dimension of the representation of both GPT and PT\nbatchSize = 64 # batch size of training data\ndecimals = 4 # decimals\nn_layers = 2\nn_heads = 4\nnumPoints = 9\nblockSize = 95\nnum_workers = 2\ndataInfo = 'Layers_{}Heads_{}EmbeddingSize{}'.format(n_layers, n_heads, embeddingSize)\naddr = './checkpoints/' # where to save model\nbestLoss = None # if there is any model to load as pre-trained one\nfName = '{}_phonon_moco.txt'.format(dataInfo)\nckptPath = '{}/{}.pt'.format(addr,fName.split('.txt')[0])\n\n\n\n\n# filtered_ids = pd.read_csv('./filtered_idx.csv')['ids'].tolist()\n\n# db = connect('/home/wy/bader/MoreH_924.db')\ndb = connect('./data/processed/structures.db')\n\n\nrows = []\nfor row in db.select():\n    rows.append(row)\n\ndataset = CharDataset(rows)\n\n\ntotal_size = len(dataset)\ntrain_size = int(total_size * 0.8)\ntest_size = int(total_size * 0.1)\n# 确保训练集、测试集和验证集的总和等于数据集的总大小\nvalidation_size = total_size - train_size - test_size\n\nindices = torch.randperm(total_size).tolist()\n# indices = [i for i in range(total_size)]\n\ntrain_indices, val_indices, test_indices = indices[: train_size], indices[train_size: train_size + validation_size], indices[train_size + validation_size :]\ntrain_dataset = Subset(dataset, train_indices)\nval_dataset = Subset(dataset, val_indices)\ntest_dataset = Subset(dataset, test_indices)\n\n\nmconf = cry_config(blockSize,\n                  n_layer=n_layers, n_head=n_heads, n_embd=embeddingSize)\ncry_encoder = CRY_ENCODER(mconf)\n\n\n\npconf = PointNetConfig(embeddingSize=embeddingSize,\n                       numberofPoints=numPoints)\nmconf = CLIPConfig(blockSize,\n                  n_layer=n_layers, n_head=n_heads, n_embd=embeddingSize)\nmodel = CLIP(mconf, pconf, cry_encoder)"
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c774acee",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,\n",
    "                    batch_size=batchSize,\n",
    "                    num_workers=0,\n",
    "                    collate_fn=MyCollator(mysql_url=db),\n",
    "                    shuffle=True,\n",
    "                    drop_last=False)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                    batch_size=2*batchSize,\n",
    "                    num_workers=0,\n",
    "                    collate_fn=MyCollator(mysql_url=db),\n",
    "                    shuffle=True,\n",
    "                    drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a1517a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "\n",
    "pbar = tqdm(train_loader, total=len(train_loader))\n",
    "for step, batch in enumerate(pbar):\n",
    "    batch = {k: v.to(device) for k, v in batch.items()}\n",
    "    loss, logits = model(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b20f8628",
   "metadata": {},
   "outputs": [],
   "source": "model.to(device)\n\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4, betas=(0.9,0.95))\n\n\nnum_train_epochs = numEpochs\nnum_warmup_epochs = 0\noutput_dir = './checkpoints'\nmodel.train()\n\n# 假设 train_loader 和 val_loader 均已定义\nnum_update_steps_per_epoch = math.ceil(len(train_loader) / 1)\nmax_train_steps = num_train_epochs * num_update_steps_per_epoch\n\nlr_scheduler = get_scheduler(\n    name='cosine',\n    optimizer=optimizer,\n    num_warmup_steps=0,\n    num_training_steps=max_train_steps,\n)\nstart_epoch = 0\ncompleted_steps = 0\n\nprint_loss = []\ncompleted_steps = 0\nbest_loss = float('inf')\n\nfor epoch in range(start_epoch, num_train_epochs):\n    model.train()\n    train_loss_sum = 0.0\n    t0 = time.time()\n\n    pbar = tqdm(train_loader, total=len(train_loader))\n    for step, batch in enumerate(pbar):\n        batch = {k: v.to(device) for k, v in batch.items()}\n        loss, logits = model(batch)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        lr_scheduler.step()\n        train_loss_sum += loss.item()\n        print_loss.append(loss.item())\n        lr = optimizer.param_groups[0]['lr']\n        pbar.set_description(f\"Epoch {epoch+1} Step {step}: loss {loss.item():.4f}, lr {lr:.2e}\")\n\n        completed_steps += 1\n\n    model.eval()\n    val_losses = []\n    with torch.no_grad():\n        for batch in val_loader:\n            batch = {k: v.to(device) for k, v in batch.items()}\n            loss, logits = model(batch)\n            val_losses.append(loss.item())\n    avg_val_loss = sum(val_losses) / len(val_losses)\n    print(f\"Epoch {epoch+1} validation loss: {avg_val_loss:.4f}  time: {time.time()-t0:.1f}s\")\n\n    # 保存最优\n    if avg_val_loss < best_loss:\n        best_loss = avg_val_loss\n        print(\"Saving best model…\")\n        state_dict = model.module.state_dict() if hasattr(model, 'module') else model.state_dict()\n        torch.save(state_dict, os.path.join(output_dir, \"best_contra.pt\"))"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}